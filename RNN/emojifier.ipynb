{ "cells": [  {   "cell_type": "code",   "execution_count": 2,   "metadata": {},   "outputs": [],   "source": [    "import numpy as np \n",    "import pandas as pd \n",    "import emoji\n",    "import matplotlib.pyplot as plt\n",    "import random\n",    "from keras.models import Model\n",    "from keras.layers import Dense, Input, Dropout, LSTM, Activation\n",    "from keras.layers import Embedding\n",    "from keras.callbacks import ReduceLROnPlateau"   ]  },  {   "cell_type": "code",   "execution_count": 3,   "metadata": {},   "outputs": [],   "source": [    "train_ds = pd.read_csv('/Users/alfahwun/Downloads/train_emoji.csv',header=None).drop([2,3],axis=1)\n",    "test_ds = pd.read_csv('/Users/alfahwun/Downloads/test_emoji.csv', header=None)"   ]  },  {   "cell_type": "code",   "execution_count": 4,   "metadata": {},   "outputs": [],   "source": [    "X_train = train_ds[0]\n",    "Y_train = train_ds[1]"   ]  },  {   "cell_type": "code",   "execution_count": 5,   "metadata": {},   "outputs": [],   "source": [    "X_test = test_ds[0]\n",    "Y_test = test_ds[1]"   ]  },  {   "cell_type": "code",   "execution_count": 6,   "metadata": {},   "outputs": [],   "source": [    "from keras.utils import to_categorical\n",    "Y_train_hot = to_categorical(Y_train)\n",    "Y_test_hot = to_categorical(Y_test)"   ]  },  {   "cell_type": "code",   "execution_count": 7,   "metadata": {},   "outputs": [],   "source": [    "emoji_dictionary = {\"0\": \":heart:\",    \n",    "                    \"1\": \":baseball:\",\n",    "                    \"2\": \":smile:\",\n",    "                    \"3\": \":disappointed:\",\n",    "                    \"4\": \":fork_and_knife:\"}\n",    "\n",    "def label_to_emoji(label):\n",    "    return emoji.emojize(emoji_dictionary[str(label)])\n"   ]  },  {   "cell_type": "code",   "execution_count": 8,   "metadata": {},   "outputs": [],   "source": [    "# we will be using 50 d word vectors\n",    "def read_glove_vecs(glove_file):\n",    "    with open(glove_file, 'r') as f:\n",    "        words = set()         # ensures unique values\n",    "        word_to_vec_map = {}  # this will be a dictionary mapping words to their vectors\n",    "        for line in f:\n",    "            line = line.strip().split()\n",    "            curr_word = line[0]\n",    "            words.add(curr_word)\n",    "            word_to_vec_map[curr_word] = np.array(line[1:], dtype=np.float64)\n",    "        \n",    "        i = 1\n",    "        words_to_index = {}   # dictionary mapping words to their index in the dictionary\n",    "        index_to_words = {}   # dictionary mapping index to the word in the dictionary\n",    "        for w in sorted(words):\n",    "            words_to_index[w] = i\n",    "            index_to_words[i] = w\n",    "            i = i + 1\n",    "    return words_to_index, index_to_words, word_to_vec_map\n"   ]  },  {   "cell_type": "code",   "execution_count": 9,   "metadata": {},   "outputs": [],   "source": [    "words_to_index, index_to_words, word_to_vec_map = read_glove_vecs('/Users/alfahwun/Downloads/glove.6B.100d.txt')"   ]  },  {   "cell_type": "code",   "execution_count": 15,   "metadata": {},   "outputs": [],   "source": [    "word_to_vec_map"   ]  },  {   "cell_type": "code",   "execution_count": 12,   "metadata": {},   "outputs": [],   "source": [    "def sentences_to_indices(X, word_to_index, max_len):\n",    " \n",    "    m = X.shape[0]                                  \n",    "    X_indices = np.zeros((m, max_len))\n",    "    \n",    "    for i in range(m):                               \n",    "        \n",    "        # Convert the ith training sentence in lower case and split is into words. You should get a list of words.\n",    "        sentence_words = X[i].lower().split()\n",    "        \n",    "        j = 0\n",    "        \n",    "        # Loop over the words of sentence_words\n",    "        for w in sentence_words:\n",    "            X_indices[i, j] = word_to_index[w]\n",    "            j = j+1  \n",    "    \n",    "    return X_indices\n"   ]  },  {   "cell_type": "code",   "execution_count": null,   "metadata": {},   "outputs": [],   "source": [    "X_train_index = sentences_to_indices(X_train,word)"   ]  },  {   "cell_type": "code",   "execution_count": null,   "metadata": {},   "outputs": [],   "source": [    "def pretrained_embedding_layer(word_to_vec_map, word_to_index):\n",    "    vocab_len = len(word_to_index) + 1               # +1 for Keras  \n",    "    emb_dim = 50                                     # dimensionality of your GloVe word vectors\n",    "    \n",    "    emb_matrix = np.zeros((vocab_len, emb_dim))      # Initialization with zeros\n",    "    \n",    "    # Set each row \"index\" of the embedding matrix to be the word vector representation of the \"index\"th word of the vocabulary\n",    "    for word, index in word_to_index.items():\n",    "        emb_matrix[index, :] = word_to_vec_map[word]\n",    "\n",    "    # Define Keras embedding layer with the correct output/input sizes\n",    "    embedding_layer = Embedding(vocab_len, emb_dim, trainable=False)\n",    "    \n",    "    # Build the embedding layer\n",    "    embedding_layer.build((None,))\n",    "    \n",    "    # Set the weights of the embedding layer to the embedding matrix. Your layer is now pretrained.\n",    "    embedding_layer.set_weights([emb_matrix])\n",    "    \n",    "    return embedding_layer"   ]  },  {   "cell_type": "code",   "execution_count": null,   "metadata": {},   "outputs": [],   "source": [    "def Emojify(input_shape, word_to_vec_map, word_to_index):\n",    "    sentence_indices = Input(shape=input_shape, dtype='int32')\n",    "    embedding_layer = pretrained_embedding_layer(word_to_vec_map, word_to_index)\n",    "    embeddings = embedding_layer(sentence_indices)\n",    "    \n"   ]  },  {   "cell_type": "code",   "execution_count": null,   "metadata": {},   "outputs": [],   "source": [    "    X = LSTM(128, return_sequences=True)(embeddings)\n",    "    X = Dropout(0.5)(X)\n",    "    X = LSTM(128, return_sequences=False)(X)\n",    "    X = Dropout(0.5)(X)\n",    "    X = Dense(5, activation='softmax')(X)\n",    "    X = Activation('softmax')(X)    \n",    "    \n",    "    model = Model(inputs=sentence_indices, outputs=X)\n",    "    \n",    "    return model\n",    "\n"   ]  },  {   "cell_type": "code",   "execution_count": null,   "metadata": {},   "outputs": [],   "source": [    "emojifier = Emojify((maxWords,), word_to_vec_map, words_to_index)\n",    "reduce_lr = ReduceLROnPlateau(monitor='loss', factor=0.5, patience=3, min_lr=0.00001, verbose=1)\n",    "emojifier.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",    "emojifier.fit(X_train_index, Y_train_hot, epochs = 100, batch_size = 16, shuffle=True, \n",    "                               callbacks=[reduce_lr])\n",    "    "   ]  } ], "metadata": {  "kernelspec": {   "display_name": "Python 3 (ipykernel)",   "language": "python",   "name": "python3"  },  "language_info": {   "codemirror_mode": {    "name": "ipython",    "version": 3   },   "file_extension": ".py",   "mimetype": "text/x-python",   "name": "python",   "nbconvert_exporter": "python",   "pygments_lexer": "ipython3",   "version": "3.10.9"  } }, "nbformat": 4, "nbformat_minor": 2}